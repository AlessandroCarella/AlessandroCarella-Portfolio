<p>
    For this class project me and my colleagues developed an
    <strong>end-to-end business intelligence solution</strong> analyzing Chicago
    traffic crash data to support a mock insurance company decision-making
    through <em>dimensional modeling</em>, <em>ETL pipelines</em>,
    <em>OLAP cubes</em>, and interactive <em>Power BI</em> dashboards.
</p>
<p>
    The complete project is available on GitHub at
    <a href="https://github.com/SaraHoxha/lds-dw-modelling">lds-dw-modelling</a
    >. Collaborating with my colleagues, we
    <strong>divided responsibilities</strong> across data cleaning, schema
    design, ETL development, OLAP implementation, and dashboards creation. The
    classes taught me the
    <strong>complete data warehousing lifecycle</strong> from raw data to
    actionable business insights and I and my colleagues applied those concepts
    in the project.
</p>
<!--  -->
<!--  -->
<h2>What I Learned</h2>
<p>
    This project was my <strong>first opportunity to apply</strong> the
    comprehensive data warehousing and OLAP concepts taught throughout the
    <em>Decision Support Systems</em> course. While the lectures provided the
    theoretical foundation, implementing a complete business intelligence
    solution from raw data to interactive dashboards revealed the
    <strong>practical complexities and trade-offs</strong> inherent in
    real-world DSS development.
</p>
<p>
    In class, we learned about <em>dimensional modeling</em>, the distinction
    between fact tables and dimension tables, the principles of
    <em>star</em> and <em>snowflake schemas</em>, and the importance of grain
    definition. When we designed our fact table, these principles came to life.
    The decision to use a
    <strong>snowflake schema rather than a pure star schema</strong> wasn't
    arbitrary, it directly applied the course material on normalization
    trade-offs.
</p>
<p>
    The lectures explained that snowflake schemas
    <strong>reduce redundancy by normalizing dimension tables</strong>, but at
    the cost of
    <strong>more complex queries requiring additional joins</strong>. Our choice
    to normalize some dimensions into more sub-dimensions showed us this
    trade-off.
</p>
<p>
    The course covered <em>ETL (Extract, Transform, Load)</em> processes and the
    critical importance of data quality in decision support systems. Lectures
    emphasized the
    <strong
        >80/20 rule: 80% of a data warehousing project's effort goes into data
        cleaning and ETL</strong
    >, with only 20% on reporting and analysis. I initially thought this was an
    exaggeration, but after spending weeks on data preprocessing,
    transformation, and quality validation, I completely understand it now.
</p>
<p>
    The <em>SSIS (SQL Server Integration Services)</em> classes taught us about
    the core transformation components: <em>Lookup</em>,
    <em>Derived Column</em>, <em>Conditional Split</em>, <em>Aggregate</em>, and
    <em>Sort</em>. In the project we used those tools in a chain to create
    <strong>complex data flows</strong>.
</p>
<p>
    The lectures on <strong>data quality dimensions</strong> turned out to be
    very useful when we handled missing values across numerous columns in the
    provided tables. The course taught us to
    <strong>document data quality decisions</strong> and establish business
    rules for handling anomalies.
</p>
<p>
    One aspect we had to figure out
    <strong>beyond the course material</strong> was the
    <em>geocoding API</em> integration for recovering missing latitude/longitude
    values. The lectures covered external data enrichment conceptually, but the
    practical challenges of API use required
    <strong>independent problem-solving</strong>.
</p>
<p>
    The <em>OLAP (Online Analytical Processing)</em> lectures introduced us to
    multidimensional data modeling, measure groups, dimensions, hierarchies, and
    the fundamental operations of
    <strong>roll-up, drill-down, slice, and dice</strong>. The course used the
    classic sales data warehouse example which provided clear conceptual
    understanding.
</p>
<p>
    The concept of <strong>calculated measures</strong>, introduced in class
    through examples like profit margins and year-over-year growth, became much
    more complex in our <em>MDX</em> queries. The course taught us the
    <em>WITH MEMBER</em> syntax and the <em>CURRENTMEMBER</em> function, but
    applying these to compute relative time calculations took significant
    experimentation. Understanding that
    <strong
        >MDX operates on dimensional contexts rather than row-by-row like
        SQL</strong
    >
    was essential, but internalizing it required writing failing queries,
    debugging, and gradually building intuition about when calculations execute
    in the MDX evaluation pipeline.
</p>
<p>
    The <em>MDX (Multidimensional Expressions)</em> portion of the course
    introduced the basic query structure: <em>SELECT</em> on COLUMNS and ROWS,
    <em>FROM</em> the cube, <em>WHERE</em> for slicing. We learned key functions
    like <em>TOPCOUNT</em>, <em>Filter</em>, <em>Hierarchize</em>,
    <em>NON EMPTY</em>, and aggregate functions (<em>SUM</em>, <em>AVG</em>,
    <em>MEDIAN</em>, <em>MAX</em>). However, the course examples were relatively
    straightforward.
</p>
<p>
    The course's final section covered
    <strong>dashboard design principles</strong>: identifying KPIs, choosing
    appropriate visualizations, enabling interactivity, and designing for
    different user roles. The lectures emphasized that
    <strong>dashboards should answer business questions</strong>, not just
    display data.
</p>
<p>
    The <strong>performance optimization</strong> required for our 257,925 crash
    records pushed us to learn about <em>indexing strategies</em>,
    <em>query execution plans</em>, and
    <em>OLAP cube processing optimization</em>. The course mentioned that
    performance matters, but we had to independently research SQL Server index
    types, understand when to use clustered versus non-clustered indexes, and
    learn about <em>SSAS partition strategies</em> for large fact tables. When
    our initial cube processing took over 10 minutes, we implemented
    <strong>incremental processing</strong>, a technique we found in Microsoft
    documentation rather than course lectures.
</p>

<!--  -->
<!--  -->
<h2>Data Understanding, Cleaning, and Enhancement</h2>
<p>
    We began with <strong>three CSV files</strong> from Chicago's traffic crash
    database extracted from the
    <a href="https://data.cityofchicago.org">Chicago Data Portal</a>:
    <tt>People.csv</tt>, <tt>Vehicles.csv</tt>, and <tt>Crashes.csv</tt>.
</p>
<p>
    The <em>People</em> table contained demographic and behavioral information
    about the peoples involved in the crashes. Across
    <strong>14 columns with missing values</strong>, incorrect data types, and
    inconsistent representations we applied
    <strong>nine cleaning operations</strong> to fill and normalize the content
    of the file.
</p>
<p>
    The <em>Vehicles</em> table provided info about crash-involved vehicles,
    including unit types, license plates, defects, and temporal information.
    Enhancements to the file focused primarily on
    <strong>consolidating missing and inconsistent values</strong>.
</p>
<p>
    The <em>Crashes</em> dataset contained spatial, temporal, and severity
    information, with several data quality improvements applied. Other than the
    usual missing data handling,
    <strong>geographic data received significant improvement</strong> through
    <em>geocoding API</em> integration, filling most missing <tt>LATITUDE</tt>,
    <tt>LONGITUDE</tt>, and <tt>POINT</tt> values using the available
    <tt>STREET_NAME</tt> and <tt>STREET_NO</tt> fields. On top of that, a
    <strong>new derived feature</strong>,
    <tt>DELTA_TIME_CRASH_DATE_POLICE_REPORT_DATE</tt>, was created to quantify
    reporting delays.
</p>
<!--  -->
<!--  -->
<h2>Snowflake Schema Design for Insurance Analytics</h2>
<p>
    During classes we learned the difference between <em>Star</em>,
    <em>Snowflake</em>, and <em>Fact Constellation</em> (otherwise named
    <em>Galaxy</em>) Schemas, and we found the
    <strong>Snowflake schema to be the best fitting</strong> for our mock
    insurance company's analytical needs. This structure
    <strong>reduced data redundancy</strong> while allowing
    <strong>efficient querying</strong> of complex relationships.
    <img src="DW Schema.png" alt="snowflake schema" />
</p>
<!--  -->
<!--  -->
<h2>Queries for Business Intelligence</h2>
<p>
    We developed <strong>four queries</strong> addressing specific insurance
    company analytical needs.
</p>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th style="width: 20%">Query</th>
            <th style="width: 30%">Purpose</th>
            <th style="width: 20%">Tables involved</th>
            <th style="width: 30%">Methodology</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>Crash Frequency by Participant</strong></td>
            <td>
                Analyzes <strong>crash involvement frequency</strong> for all
                participants across different years to
                <strong
                    >identify individuals with higher crash frequencies</strong
                >.
            </td>
            <td>
                <ul>
                    <li><em>Person</em></li>
                    <li><em>DamageReimbursement</em></li>
                    <li><em>Crash</em></li>
                    <li><em>DateTime</em></li>
                </ul>
            </td>
            <td>
                Extracted demographic details, linked to retrieve crash IDs,
                joined to extract crash years. Data aggregated by counting
                crashes per participant per year, generating
                <strong>"Total Crashes" metric</strong>, sorted chronologically.
            </td>
        </tr>
        <tr>
            <td><strong>Day-Night Crash Index by Police Beat</strong></td>
            <td>
                Calculates a <strong>day-night crash index</strong> per police
                beat by comparing vehicle counts in nighttime incidents (9 PM to
                8 AM) versus daytime (8 AM to 9 PM).
            </td>
            <td>
                <ul>
                    <li><em>Crash</em> (Number_of_Units)</li>
                    <li><em>CrashLocation</em> (Beat_of_Occurrence)</li>
                    <li><em>DateTime</em> (timestamps)</li>
                </ul>
            </td>
            <td>
                Created <em>Time_Category</em> field classifying crashes as
                "Day" or "Night". Data split by Time_Category, aggregated by
                Beat_of_Occurrence, summed vehicle counts. Final
                <strong>ratio calculation handled division by zero</strong>
                cases.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Age-Based Crash Ratios by Location and Weather</strong>
            </td>
            <td>
                Calculates the
                <strong>ratio of people under 21 to those over 21</strong> for
                each quarter, weather condition, and beat of occurrence.
                Analyzes how
                <strong
                    >seasonality, weather, and location affect crash
                    proportions</strong
                >
                by age.
            </td>
            <td>
                <ul>
                    <li><em>DamageReimbursement</em></li>
                    <li><em>Crash</em></li>
                    <li><em>CrashLocation</em></li>
                    <li><em>CrashCondition</em></li>
                    <li><em>DateTime</em></li>
                    <li><em>Person</em></li>
                </ul>
            </td>
            <td>
                Used <em>SSIS lookup</em> operations to join tables. Counted
                individuals over and under 21, aggregated by required columns
                (quarter, weather, location), and created ratio column.
            </td>
        </tr>
        <tr>
            <td>
                <strong
                    >Interstate Movement Analysis for Elderly Drivers</strong
                >
            </td>
            <td>
                Examines the
                <strong>ratio of older individuals (over 60)</strong> involved
                in interstate movements in Chicago, analyzing residence location
                and vehicle registration state. Incorporates
                <strong>quarterly variations to capture seasonality</strong>.
            </td>
            <td>
                <ul>
                    <li>
                        Multiple tables (via <em>lookup transformations</em>)
                    </li>
                    <li>
                        Focus on <em>Person</em> and vehicle registration data
                    </li>
                </ul>
            </td>
            <td>
                Used lookup transformations to retrieve necessary attributes.
                Classified individuals as over/under 60, aggregated by quarter
                and license plate registration, calculated ratio, and sorted
                results.
            </td>
        </tr>
    </tbody>
</table>
<!--  -->
<!--  -->
<h2>OLAP Cube Implementation</h2>
<p>
    We designed a <strong>multidimensional OLAP cube</strong> using
    <em>SQL Server Analysis Services (SSAS)</em> to enable sophisticated
    analytical queries. The cube stores and analyzes Chicago traffic crash data
    using dimensions for <em>Person</em>, <em>Crash</em>, <em>Vehicle</em>, and
    <em>DateTime</em>.
</p>
<h3>Dimension Design with Hierarchies</h3>
<p>
    The <strong>Person dimension</strong> includes attributes like
    <tt>Person_ID</tt>, <tt>Injury_Classification</tt>,
    <tt>Physical_Condition</tt>, <tt>Age</tt>, <tt>Type</tt>, <tt>Sex</tt>,
    <tt>Driver_Action</tt>, <tt>Driver_Vision</tt>, <tt>Age_Group</tt>,
    <tt>City</tt>, and <tt>State</tt>.
</p>
<p>
    The <strong>Crash dimension</strong> has a complex structure with
    <tt>Crash_ID</tt>, <tt>Crash_Date_ID</tt>, <tt>Police_Notified_Date_ID</tt>,
    <tt>Crash_Location_ID</tt>, <tt>Crash_Condition_ID</tt>, <tt>Injury_ID</tt>,
    <tt>Primary_Contributory_Cause</tt>, <tt>Secondary_Contributory_Cause</tt>,
    and various location attributes.
</p>
<p>
    The <strong>Vehicle dimension</strong> contains <tt>Vehicle_ID</tt> and
    <tt>Vehicle_Type</tt>. The <strong>DateTime dimension</strong> includes
    <tt>Date_Time_ID</tt>, <tt>Day</tt>, <tt>Year</tt>, <tt>Month_Name</tt>, and
    <tt>Time</tt> in 24-hour format.
</p>
<p>
    The <em>DateTime</em> dimension includes a <strong>hierarchy</strong>:
    <tt>Year</tt> → <tt>Month</tt> → <tt>Day</tt> → <tt>Time</tt>. Following
    OLAP best practices, access to these hierarchical attributes is not directly
    visible to end-users.
</p>
<!--  -->
<h3>Measure Groups</h3>
<p>
    The <strong>Damage Reimbursement measure group</strong> captures financial
    data including <tt>Cost</tt>, <tt>Damage_Reimbursement_Count</tt>, and
    <tt>Average_Cost</tt>, essential for
    <strong>assessing financial impact</strong>. The
    <strong>Person measure group</strong> provides insights into the number and
    type of individuals involved, including <tt>Person_Count</tt>,
    <tt>Count_of_Person_Type</tt>, and <tt>Fatal_Crashes</tt>.
</p>
<p>
    After building and defining the structure, we
    <strong>processed the data and deployed the cube</strong> on the OLAP
    server, enabling MDX queries and analysis directly within the cube
    environment.
</p>
<h2>Advanced MDX Query Development</h2>
<p>
    We developed <strong>five MDX queries</strong> demonstrating different
    analytical capabilities.
</p>

<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th style="width: 20%">Query</th>
            <th style="width: 30%">Purpose</th>
            <th style="width: 20%">Tables Involved</th>
            <th style="width: 30%">Methodology</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>
                <strong>Total Damage Costs by Location and Month</strong>
            </td>
            <td>
                Shows <strong>total damage costs</strong> for each location and
                month with grand totals.
            </td>
            <td>
                <ul>
                    <li><em>Damage Cost</em> measure</li>
                    <li><em>Location</em> dimension</li>
                    <li><em>Month/Year</em> dimension</li>
                </ul>
            </td>
            <td>
                Used <em>WITH MEMBER</em> to define custom grand totals for
                months and locations. <em>Hierarchize</em> function orders
                months chronologically with yearly total.
                <em>Filter</em> excludes All member for locations, showing only
                individual locations plus grand total.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Year-over-Year Damage Cost Changes</strong>
            </td>
            <td>
                Shows
                <strong>damage cost changes percentage-wise</strong> relative to
                the previous year for every location.
            </td>
            <td>
                <ul>
                    <li><em>Damage Cost</em> measure</li>
                    <li><em>Location</em> dimension</li>
                    <li><em>Year</em> dimension (2017)</li>
                </ul>
            </td>
            <td>
                Two <em>WITH MEMBER</em> clauses: one to name damage cost of
                previous year, another to create
                <strong>percentage change measure</strong>. Displayed 2017 data
                with cost and percentage change on columns, crash location in
                rows.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Highest Damage by Vehicle Type and Year</strong>
            </td>
            <td>
                Shows for each vehicle type and year the information and total
                damage costs of the
                <strong>person with the highest reported damage</strong>.
            </td>
            <td>
                <ul>
                    <li><em>Total Cost</em> measure</li>
                    <li><em>Person</em> dimension</li>
                    <li><em>Vehicle Type</em> dimension</li>
                    <li><em>Year</em> dimension</li>
                </ul>
            </td>
            <td>
                Selected Total Cost as sum of each current member of Person.
                Rows use nested function with non-empty values, generating
                year-vehicle type combinations, then extracting
                <strong>top 1 person per combination</strong> measured by cost.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Median and Maximum Reporting Delays</strong>
            </td>
            <td>
                Calculates
                <strong>median and maximum time delays</strong> between crash
                occurrence and police notification for each beat of occurrence,
                providing insights into typical and extreme reporting delays.
            </td>
            <td>
                <ul>
                    <li>
                        <em
                            >Difference_Between_Crash_Date_And_Police_Notified</em
                        >
                        measure
                    </li>
                    <li><em>Beat of Occurrence</em> dimension</li>
                </ul>
            </td>
            <td>
                Defined <em>TotalDelaySeconds</em> to compute total delay by
                extracting and converting components (days, hours, minutes,
                seconds). Calculated median and maximum delays using
                <em>MEDIAN</em> and <em>MAX</em> functions, formatted into
                human-readable strings with null handling.
                <strong>Sorted by median delay descending</strong>.
            </td>
        </tr>
        <tr>
            <td>
                <strong
                    >Most Frequent Crash Causes with Weighted Analysis</strong
                >
            </td>
            <td>
                Identifies the <strong>most frequent crash cause</strong> for
                each year, calculates associated total damage costs, and
                determines the
                <strong>overall most frequent crash cause</strong>.
            </td>
            <td>
                <ul>
                    <li><em>Primary Crash Cause</em> dimension</li>
                    <li><em>Secondary Crash Cause</em> dimension</li>
                    <li><em>Total Damage Cost</em> measure</li>
                    <li><em>Year</em> dimension</li>
                </ul>
            </td>
            <td>
                <strong
                    >Weighted primary crash factor twice as much as
                    secondary</strong
                >
                factor using <em>WeightedCauseCount</em>. Used
                <em>TOPCOUNT</em> to determine most frequent cause per year by
                weighted frequency. Aggregated TotalCostPerYearTopCause for top
                cause. Identified overall most frequent cause via TOPCOUNT
                without time restrictions.
            </td>
        </tr>
    </tbody>
</table>
<!--  -->
<!--  -->
<h2>Interactive Dashboard Development</h2>
<p>
    We created <strong>three comprehensive dashboards</strong> in
    <em>Power BI</em> to visualize key insights for insurance company
    stakeholders.
</p>
<!--  -->
<h3>Geographical Dashboard: Spatial Analysis of Damage Costs</h3>
<p>
    The dashboard showcases the
    <strong>distribution of total damage costs by vehicle category</strong> with
    interactive filtering capabilities. Two primary filters on the left allow
    users to select specific beats of occurrence and vehicle types dynamically.
    The middle column features a <strong>treemap</strong> at the top
    highlighting total damage costs for selected vehicle categories, providing a
    hierarchical overview of cost distributions. Below, a
    <strong>bar plot</strong> displays the same data with beat of occurrence
    comparison context in a detailed format.
</p>
<p>
    On the right, a <strong>map visualization</strong> provides spatial
    insights, focusing on the Chicago area by default when all beats are
    selected. The map
    <strong>dynamically updates to reflect chosen beats</strong>, allowing
    exploration of specific regions and identification of areas with higher
    damage cost concentrations. Users can manipulate filters to adapt displayed
    data, enabling nuanced analyses of relationships between geographical
    locations and vehicle-related damage costs.
    <img src="geo_dash.png" alt="Geographical Dashboard" />
</p>
<h3>Streets Dashboard: Mobility and Location Analysis</h3>
<p>
    The dashboard provides insights into
    <strong>cost and damage reimbursement by street</strong>, helping the
    insurance company locate and analyze mobility dynamics when granting
    insurance agreements or paying damages. The visualizations include total
    cost metrics, <strong>average cost increase by year</strong>, count of
    damage reimbursements by street, and a street map showing the
    <strong>top 10 streets with highest costs</strong> in Chicago. These
    visualizations together help determine new policies for the insurance
    company by revealing patterns in street-level crash costs and frequencies.
    <img src="Street_dashboard.jpeg" alt="Street Dashboard" />
</p>
<!--  -->
<h3>People Dashboard: Demographic and Behavioral Analysis</h3>
<p>
    The <em>People</em> dashboard provides
    <strong>actionable information about crash participants</strong> for
    business analysts. At the top left, a pie chart titled "Proportion of Crash
    Participants by Physical Condition" identifies the
    <strong>impact of factors such as impairment</strong> due to drugs, alcohol,
    or medical conditions, helping understand the role of human factors. The map
    visualizes geographical concentrations of crashes, allowing insurers to
    <strong>identify high-risk regions</strong> for adjustments.
</p>
<p>
    The bottom left bar chart provides insight into
    <strong>crash costs by age group and vision impairments</strong>, revealing
    that younger drivers (under 18) and those with specific vision conditions
    incur higher costs, identifying
    <strong>high-risk demographics for targeted pricing</strong>. The treemap
    "Driver Action by Injury Breakdown" uncovers how specific driver behaviors
    (failure to act, distracted driving) correlate with injuries, showing
    potential needs to adjust premiums for policyholders exhibiting these
    patterns.
</p>
<p>
    Central KPIs display <strong>Participants in Crashes</strong>,
    <strong>Total Damage Cost</strong>, <strong>Fatal Crashes</strong>, and
    <strong>Average Damage Cost</strong>, providing an immediate overview of
    claim volume and financial exposure crucial for forecasting payments and
    ensuring sufficient reserves.
    <img src="People_Dashboard.png" alt="People Dashboard" width="100%" />
</p>
